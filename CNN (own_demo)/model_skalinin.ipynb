{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def shuffle_list(images_list, truth_list, dir_list):\n",
    "    list_storage = zip(images_list, truth_list, dir_list)\n",
    "    list_storage = list(list_storage)\n",
    "    np.random.shuffle(list_storage)\n",
    "    images_list, truth_list, dir_list = zip(*list_storage) # https://stackoverflow.com/questions/12974474/how-to-unzip-a-list-of-tuples-into-individual-lists\n",
    "    return images_list, truth_list, dir_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_axis_indexes(size_axis, center_w_l):\n",
    "    coordinates = []\n",
    "    for i in range(-center_w_l, size_axis-center_w_l):\n",
    "        coordinates.append(i)\n",
    "    return coordinates\n",
    "\n",
    "def create_indexes(size_axis, center_w_l):\n",
    "    # расчет координат на осях ядра свертки в зависимости от номера центрального элемента ядра\n",
    "    coordinates_a = create_axis_indexes(size_axis=size_axis[0], center_w_l=center_w_l[0])\n",
    "    coordinates_b = create_axis_indexes(size_axis=size_axis[1], center_w_l=center_w_l[1])\n",
    "    return coordinates_a, coordinates_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def convolution_feed_x_l(y_l_minus_1, w_l, conv_params):\n",
    "    indexes_a, indexes_b = create_indexes(size_axis=w_l.shape, center_w_l=conv_params['center_w_l'])\n",
    "    stride = conv_params['stride']\n",
    "    # матрица выхода будет расширяться по мере добавления новых элементов\n",
    "    x_l = np.zeros((1,1))\n",
    "    # в зависимости от типа операции меняется основная формула функции\n",
    "    if conv_params['convolution']:\n",
    "        g = 1 # операция конволюции\n",
    "    else:\n",
    "        g = -1 # операция корреляции\n",
    "    # итерация по i и j входной матрицы y_l_minus_1 из предположения, что размерность выходной матрицы x_l будет такой же\n",
    "    for i in range(y_l_minus_1.shape[0]):\n",
    "        for j in range(y_l_minus_1.shape[1]):\n",
    "            #demo = np.zeros([y_l_minus_1.shape[0], y_l_minus_1.shape[1]]) # матрица для демонстрации конволюции\n",
    "            result = 0\n",
    "            element_exists = False\n",
    "            for a in indexes_a:\n",
    "                for b in indexes_b:\n",
    "                    # проверка, чтобы значения индексов не выходили за границы\n",
    "                    if i*stride - g*a >= 0 and j*stride - g*b >= 0 \\\n",
    "                    and i*stride - g*a < y_l_minus_1.shape[0] and j*stride - g*b < y_l_minus_1.shape[1]:\n",
    "                        result += y_l_minus_1[i*stride - g*a][j*stride - g*b] * w_l[indexes_a.index(a)][indexes_b.index(b)] # перевод индексов в \"нормальные\" для извлечения элементов из матрицы w_l\n",
    "                        # demo[i*stride - g*a][j*stride - g*b] = w_l[indexes_a.index(a)][indexes_b.index(b)]\n",
    "                        element_exists = True\n",
    "            # запись полученных результатов только в том случае, если для данных i и j были произведены вычисления\n",
    "            if element_exists:\n",
    "                if i >= x_l.shape[0]:\n",
    "                    # добавление строки, если не существует\n",
    "                    x_l = np.vstack((x_l, np.zeros(x_l.shape[1])))\n",
    "                if j >= x_l.shape[1]:\n",
    "                    # добавление столбца, если не существует\n",
    "                    x_l = np.hstack((x_l, np.zeros((x_l.shape[0],1))))\n",
    "                x_l[i][j] = result\n",
    "                # вывод матрицы demo для отслеживания хода свертки\n",
    "                # print('i=' + str(i) + '; j=' + str(j) + '\\n', demo)\n",
    "    return x_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def maxpool(y_l, conv_params):\n",
    "    indexes_a, indexes_b = create_indexes(size_axis=conv_params['window_shape'], center_w_l=conv_params['center_window'])\n",
    "    stride = conv_params['stride']\n",
    "    # выходные матрицы будут расширяться по мере добавления новых элементов\n",
    "    y_l_mp = np.zeros((1,1)) # матрица y_l после операции макспулинга\n",
    "    y_l_mp_to_y_l = np.zeros((1,1), dtype='<U32') # матрица для backprop через слой макспулинга (внутри матрицы будет храниться текст)\n",
    "    # в зависимости от типа операции меняется основная формула функции\n",
    "    if conv_params['convolution']:\n",
    "        g = 1 # операция конволюции\n",
    "    else:\n",
    "        g = -1 # операция корреляции\n",
    "    # итерация по i и j входной матрицы y_l из предположения, что размерность выходной матрицы будет такой же\n",
    "    for i in range(y_l.shape[0]):\n",
    "        for j in range(y_l.shape[1]):\n",
    "            result = -np.inf\n",
    "            element_exists = False\n",
    "            for a in indexes_a:\n",
    "                for b in indexes_b:\n",
    "                    # проверка, чтобы значения индексов не выходили за границы\n",
    "                    if i*stride - g*a >= 0 and j*stride - g*b >= 0 \\\n",
    "                    and i*stride - g*a < y_l.shape[0] and j*stride - g*b < y_l.shape[1]:\n",
    "                        if y_l[i*stride - g*a][j*stride - g*b] > result:\n",
    "                            result = y_l[i*stride - g*a][j*stride - g*b]\n",
    "                            i_back = i*stride - g*a\n",
    "                            j_back = j*stride - g*b\n",
    "                        element_exists = True\n",
    "            # запись полученных результатов только в том случае, если для данных i и j были произведены вычисления\n",
    "            if element_exists:\n",
    "                if i >= y_l_mp.shape[0]:\n",
    "                    # добавление строки, если не существует\n",
    "                    y_l_mp = np.vstack((y_l_mp, np.zeros(y_l_mp.shape[1])))\n",
    "                    # матрица y_l_mp_to_y_l расширяется соответственно матрице y_l_mp\n",
    "                    y_l_mp_to_y_l = np.vstack((y_l_mp_to_y_l, np.zeros(y_l_mp_to_y_l.shape[1])))\n",
    "                if j >= y_l_mp.shape[1]:\n",
    "                    # добавление столбца, если не существует\n",
    "                    y_l_mp = np.hstack((y_l_mp, np.zeros((y_l_mp.shape[0],1))))\n",
    "                    y_l_mp_to_y_l = np.hstack((y_l_mp_to_y_l, np.zeros((y_l_mp_to_y_l.shape[0],1))))\n",
    "                y_l_mp[i][j] = result\n",
    "                # в матрице y_l_mp_to_y_l хранятся координаты значений,\n",
    "                # которые соответствуют выбранным в операции максипулинга ячейкам из матрицы y_l\n",
    "                y_l_mp_to_y_l[i][j] = str(i_back) + ',' + str(j_back)\n",
    "    return y_l_mp, y_l_mp_to_y_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def maxpool_feed(y_l, conv_params):\n",
    "    list_of_y_l_mp = []\n",
    "    list_of_y_l_mp_to_y_l = []\n",
    "    for i in range(len(y_l)): # итерация по всем feature map в y_l\n",
    "        y_l_mp, y_l_mp_to_y_l = maxpool(y_l[i], conv_params)\n",
    "        # выход функции, матрица y_l после прохождения операции макспулинга\n",
    "        list_of_y_l_mp.append(y_l_mp)\n",
    "        # здесь хранятся координаты, которые позволят перевести \"маленькую\" матрицу dE/dy_l_mp к \"большой\" исходной матрице dE/dy_l\n",
    "        list_of_y_l_mp_to_y_l.append(y_l_mp_to_y_l)\n",
    "    return list_of_y_l_mp, list_of_y_l_mp_to_y_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def maxpool_back(dEdy_l_mp, y_l_mp_to_y_l, y_l_shape):\n",
    "    list_of_dEdy_l = []\n",
    "    for i in range(len(dEdy_l_mp)): # операция выполняется для каждой из feature map\n",
    "        dEdy_l = np.zeros(y_l_shape) # матрица dEdy_l будет далее постепенно заполнятся значениями\n",
    "        # проход по всем элементам матрицы dEdy_l_mp\n",
    "        for k in range(dEdy_l_mp[i].shape[0]):\n",
    "            for l in range(dEdy_l_mp[i].shape[1]):\n",
    "                # каждый элемент матрицы dEdy_l_mp необходимо поставить в матрицу dEdy_l\n",
    "                # для этого извлекаем необходимые координаты \"назначения\" из матрицы y_l_mp_to_y_l\n",
    "                coordinates = y_l_mp_to_y_l[i][k][l] # коордианты выглядят так: 2,4 - то есть 2-ая строка и 4-ый столбец\n",
    "                coordinate_row = int(coordinates[:coordinates.find(',')])\n",
    "                coordinate_col = int(coordinates[coordinates.find(',')+1:])\n",
    "                # запись по этим коордианатам в матрицу dEdy_l элемента из матрицы dEdy_l_mp\n",
    "                dEdy_l[coordinate_row][coordinate_col] = dEdy_l_mp[i][k][l]\n",
    "        list_of_dEdy_l.append(dEdy_l) # добавляем получившуюся dEdy_l в лист с остальными feature map\n",
    "    return list_of_dEdy_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def convolution_back_dEdw_l(y_l_minus_1, w_l_shape, dEdx_l, conv_params):\n",
    "    indexes_a, indexes_b = create_indexes(size_axis=w_l_shape, center_w_l=conv_params['center_w_l'])\n",
    "    stride = conv_params['stride']\n",
    "    dEdw_l = np.zeros((w_l_shape[0], w_l_shape[1]))\n",
    "    # в зависимости от типа операции меняется основная формула функции\n",
    "    if conv_params['convolution']:\n",
    "        g = 1 # операция конволюции\n",
    "    else:\n",
    "        g = -1 # операция корреляции\n",
    "    # итерация по a и b ядра свертки\n",
    "    for a in indexes_a:\n",
    "        for b in indexes_b:\n",
    "            # размерность матрицы для демонстрации конволюции равноа размерности y_l, так как эта матрица либо равна либо больше (в случае stride>1) матрицы x_l\n",
    "            # demo = np.zeros([y_l_minus_1.shape[0], y_l_minus_1.shape[1]])\n",
    "            result = 0\n",
    "            for i in range(dEdx_l.shape[0]):\n",
    "                for j in range(dEdx_l.shape[1]):\n",
    "                    # проверка, чтобы значения индексов не выходили за границы\n",
    "                    if i*stride - g*a >= 0 and j*stride - g*b >= 0 \\\n",
    "                    and i*stride - g*a < y_l_minus_1.shape[0] and j*stride - g*b < y_l_minus_1.shape[1]:\n",
    "                        result += y_l_minus_1[i*stride - g*a][j*stride - g*b] * dEdx_l[i][j]\n",
    "                        # demo[i*stride - g*a][j*stride - g*b] = dEdx_l[i][j]\n",
    "            dEdw_l[indexes_a.index(a)][indexes_b.index(b)] = result # перевод индексов в \"нормальные\" для извлечения элементов из матрицы w_l\n",
    "            # вывод матрицы demo для отслеживания хода свертки\n",
    "            # print('a=' + str(a) + '; b=' + str(b) + '\\n', demo)\n",
    "    return dEdw_l\n",
    "\n",
    "def convolution_back_dEdy_l_minus_1(dEdx_l, w_l, y_l_minus_1_shape, conv_params):\n",
    "    indexes_a, indexes_b = create_indexes(size_axis=w_l.shape, center_w_l=conv_params['center_w_l'])\n",
    "    stride = conv_params['stride']\n",
    "    dEdy_l_minus_1 = np.zeros((y_l_minus_1_shape[0], y_l_minus_1_shape[1]))\n",
    "    # в зависимости от типа операции меняется основная формула функции\n",
    "    if conv_params['convolution']:\n",
    "        g = 1 # операция конволюции\n",
    "    else:\n",
    "        g = -1 # операция корреляции\n",
    "    for i in range(dEdy_l_minus_1.shape[0]):\n",
    "        for j in range(dEdy_l_minus_1.shape[1]):\n",
    "            result = 0\n",
    "            # матрица для демонстрации конволюции\n",
    "            # demo = np.zeros([dEdx_l.shape[0], dEdx_l.shape[1]])\n",
    "            for i_x_l in range(dEdx_l.shape[0]):\n",
    "                for j_x_l in range(dEdx_l.shape[1]):\n",
    "                    # перевод индексов в \"нормальные\" для извлечения элементов из матрицы w_l\n",
    "                    a = g*i_x_l*stride - g*i\n",
    "                    b = g*j_x_l*stride - g*j\n",
    "                    # проверка на вхождение в диапазон индексов ядра свертки\n",
    "                    if a in indexes_a and b in indexes_b:\n",
    "                        a = indexes_a.index(a)\n",
    "                        b = indexes_b.index(b)\n",
    "                        result += dEdx_l[i_x_l][j_x_l] * w_l[a][b]\n",
    "                        # demo[i_x_l][j_x_l] = w_l[a][b]\n",
    "            dEdy_l_minus_1[i][j] = result\n",
    "            # вывод матрицы demo для отслеживания хода свертки\n",
    "            # print('i=' + str(i) + '; j=' + str(j) + '\\n', demo)\n",
    "    return dEdy_l_minus_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conv_weights_init(shape, quantity, weights_name, dir_npy):\n",
    "    try:\n",
    "        weights_matrix = np.load(dir_npy).item().get(weights_name)\n",
    "        print('веса для', weights_name, 'подгружены', len(weights_matrix)*weights_matrix[0].size)\n",
    "    except:\n",
    "        weights_matrix = []\n",
    "        for i in range(quantity):\n",
    "            weights_matrix.append(2 * np.random.random(shape) - 1)\n",
    "        print('веса для', weights_name, 'созданы', len(weights_matrix)*weights_matrix[0].size)\n",
    "    return weights_matrix\n",
    "\n",
    "def fc_weights_init(shape, weights_name, dir_npy):\n",
    "    try:\n",
    "        weights_matrix = np.load(dir_npy).item().get(weights_name)\n",
    "        print('веса для', weights_name, 'подгружены', weights_matrix.size)\n",
    "    except:\n",
    "        weights_matrix = 2 * np.random.random(shape) - 1\n",
    "        print('веса для', weights_name, 'созданы', weights_matrix.size)\n",
    "    return weights_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_start_step(dir_npy):\n",
    "    try:\n",
    "        # к n-ому шагу прибавляем единицу, т.к. веса сохранились после прохождения backprop и, соответственно, полного завершения шага\n",
    "        start_step = np.load(dir_npy).item().get('step') + 1\n",
    "    except:\n",
    "        start_step = 0\n",
    "    return start_step\n",
    "\n",
    "def get_saved(list_name, dir_npy):\n",
    "    try:\n",
    "        list_save = np.load(dir_npy).item().get(list_name)\n",
    "    except:\n",
    "        list_save = []\n",
    "    return list_save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def convolution_feed(y_l_minus_1, w_l, w_l_name, w_shape_l, b_l, b_l_name, feature_maps, act_fn, dir_npy, conv_params):\n",
    "    x_l = []\n",
    "    y_l = []\n",
    "    if not w_l:\n",
    "        # инициализация w_l (количество ядер свертки равно число входов умножить на количество выходов)\n",
    "        w_l = conv_weights_init(shape=w_shape_l, quantity=feature_maps*len(y_l_minus_1), weights_name=w_l_name, dir_npy=dir_npy)\n",
    "    for i in range(len(y_l_minus_1)): # для всех y_l_minus_1\n",
    "        for j in range(i*feature_maps, (i + 1)*feature_maps):\n",
    "            # для каждой y_l_minus_1 функция конволюции вызывается feature_maps раз для создания \"промежуточных\" x_l\n",
    "            x_l.append(convolution_feed_x_l(y_l_minus_1=y_l_minus_1[i], w_l=w_l[j], conv_params=conv_params))\n",
    "    if len(b_l) == 0:\n",
    "        # инициализация b_l (количество b_l равно числу выходов)\n",
    "        b_l = conv_weights_init(shape=(1,1), quantity=feature_maps, weights_name=b_l_name, dir_npy=dir_npy)\n",
    "    x_l_final = []\n",
    "    for i in range(feature_maps): # итерация по количеству выходов\n",
    "        x_l_final.append(0)\n",
    "        for j in range(len(y_l_minus_1)): # итерация по количеству входных каналов\n",
    "            x_l_final[-1] += x_l[j*feature_maps + i] # \"финальный\" x_l_final является суммой \"промежуточных\" x_l, полученных с каждой y_l_minus_1\n",
    "        x_l_final[-1] += b_l[len(x_l_final)-1] # к x_l_final прибавляем соответствующий ему bias\n",
    "        y_l.append(activation_fn(x_l_final[-1], fn_name=act_fn, feed=True)) # функция активации\n",
    "    return y_l, w_l, b_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fc_multiplication(y_l_minus_1, w_l, w_l_name, b_l, b_l_name, neurons, act_fn, dir_npy):\n",
    "    if w_l.size == 0:\n",
    "        w_l = fc_weights_init(shape=(y_l_minus_1.shape[1], neurons), weights_name=w_l_name, dir_npy=dir_npy)\n",
    "        b_l = fc_weights_init(shape=(1, neurons), weights_name=b_l_name, dir_npy=dir_npy)\n",
    "    x_l = np.dot(y_l_minus_1, w_l) + b_l\n",
    "    y_l = activation_fn(x_l, fn_name=act_fn, feed=True)\n",
    "    return y_l, w_l, b_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def activation_fn(matix, fn_name, feed):\n",
    "    output_matix = np.copy(matix)\n",
    "    if feed:\n",
    "        if fn_name == 'sigmoid':\n",
    "            output_matix = 1 / (1+np.exp(-output_matix)) # похоже, сообщение об ошибке можно проигнорировать https://stackoverflow.com/questions/23128401/overflow-error-in-neural-networks-implementation\n",
    "        if fn_name == 'relu':\n",
    "            output_matix[output_matix<0] = 0\n",
    "        if fn_name == 'softmax':\n",
    "            output_matix = np.exp(output_matix) / np.exp(output_matix).sum()\n",
    "    else:\n",
    "        if fn_name == 'sigmoid':\n",
    "            output_matix = output_matix * (1 - output_matix)\n",
    "        if fn_name == 'relu': # relu для backprop рассчитывается исходя из того, что на вход подается y_l, а не x_l\n",
    "            # output_matix[output_matix<=0] = 0 # после relu в матрице y_l не существует отрицательных значений\n",
    "            output_matix[output_matix>0] = 1\n",
    "        if fn_name == 'softmax':\n",
    "            input_matix = np.copy(matix)\n",
    "            output_matix = np.zeros((matix.shape[1], matix.shape[1]))\n",
    "            for i in range(output_matix.shape[1]):\n",
    "                for j in range(output_matix.shape[1]):\n",
    "                    if i==j:\n",
    "                        output_matix[i][i] = input_matix[0][i]*(1 - input_matix[0][i])\n",
    "                    else:\n",
    "                        output_matix[i][j] = - input_matix[0][i]*input_matix[0][j]\n",
    "    return output_matix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loss_fn(y_ground_truth, y_predicted, feed):\n",
    "    if feed:\n",
    "        # error_matix = (1/2)*(y_ground_truth - y_predicted)**2 # заменил на cross-entropy\n",
    "        error_matix = - y_ground_truth * np.log(y_predicted)\n",
    "    else:\n",
    "        # error_matix = y_predicted - y_ground_truth\n",
    "        error_matix = - (y_ground_truth/y_predicted)\n",
    "    return error_matix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def matrix2vector(matrix):\n",
    "    # функция объединения матриц в вектор\n",
    "    vector = np.array([[]])\n",
    "    for i in range(len(matrix)):\n",
    "        reshaped_matrix = np.reshape(matrix[i], (1, matrix[i].shape[0]*matrix[i].shape[1]))\n",
    "        vector = np.hstack((vector, reshaped_matrix))\n",
    "    return vector\n",
    "\n",
    "def vector2matrix(vector, matrix_shape):\n",
    "    # функция разбиения вектора на матрицы\n",
    "    matrices = []\n",
    "    matrix_size = matrix_shape[0]*matrix_shape[1]\n",
    "    for i in range(0, vector.size, matrix_size):\n",
    "        matrix = np.reshape(vector[0][i:i+matrix_size], matrix_shape)\n",
    "        matrices.append(matrix)\n",
    "    return matrices\n",
    "\n",
    "def matrix2vector_tf(matrix):\n",
    "    # функция объединения матриц в вектор\n",
    "    # объединение построено таким образом, чтобы можно было сравнить результаты с tensorflow\n",
    "    matrices = []\n",
    "    vector = np.array([])\n",
    "    for i in range(len(matrix)):\n",
    "        matrices.append(np.reshape(matrix[i], (1, matrix[i].size)))\n",
    "    for i in range(matrices[0].size): # матрица все одного размера, выбираем любую\n",
    "        for j in range(len(matrices)):\n",
    "            vector = np.hstack((vector, matrices[j][0][i]))\n",
    "    vector = np.reshape(vector, (1, vector.size))\n",
    "    return vector\n",
    "\n",
    "def vector2matrix_tf(vector, matrix_shape):\n",
    "    # функция разбиения вектора на матрицы\n",
    "    # разбиение построено таким образом, чтобы можно было сравнить результаты с tensorflow\n",
    "    matrices = []\n",
    "    for i in range(int(vector.size/(matrix_shape[0]*matrix_shape[1]))):\n",
    "        matrices.append(np.array([]))\n",
    "    j = 0\n",
    "    for i in range(vector.size):\n",
    "        matrices[j] = np.hstack((matrices[j], vector[0][i]))\n",
    "        j += 1\n",
    "        if j == len(matrices):\n",
    "            j = 0\n",
    "    for i in range(len(matrices)):\n",
    "        matrices[i] = np.reshape(matrices[i], matrix_shape)\n",
    "    return matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fc_backpropagation(y_l_minus_1, dEdy_l, y_l, w_l, b_l, act_fn, alpha):\n",
    "    # вычисление dE/dx_l, то есть backprop через функцию активации\n",
    "    if act_fn == 'softmax':\n",
    "        dEdx_l = np.dot(dEdy_l, activation_fn(y_l, fn_name=act_fn, feed=False))\n",
    "    else:\n",
    "        dEdx_l = dEdy_l * activation_fn(y_l, fn_name=act_fn, feed=False)\n",
    "    # вычисление частных производных\n",
    "    dEdw_l = np.dot(y_l_minus_1.T, dEdx_l)\n",
    "    dEdb_l = dEdx_l\n",
    "    dEdy_l_minus_1 = np.dot(dEdx_l, w_l.T)\n",
    "    # обновление матриц весов\n",
    "    w_l = w_l - alpha * dEdw_l\n",
    "    b_l = b_l - alpha * dEdb_l\n",
    "    return dEdy_l_minus_1, w_l, b_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def convolution_backpropagation(y_l_minus_1, y_l, w_l, b_l, dEdy_l, feature_maps, act_fn, alpha, conv_params):\n",
    "    list_of_dEdy_l_minus_1 = []\n",
    "    list_of_dEdx_l = []\n",
    "    for i in range(len(y_l)):\n",
    "        # сначала происходит расчет dEdx_l, то есть обратное прохождение dEdy_l через функцию активации\n",
    "        list_of_dEdx_l.append(dEdy_l[i] * activation_fn(y_l[i], fn_name=act_fn, feed=False))\n",
    "        # вследствие того, что только одна b_l приходится на одну карту признаков, то dEdb_l является суммой по всем элементам dEdx_l\n",
    "        dEdb_l = list_of_dEdx_l[-1].sum()\n",
    "        b_l[i] = b_l[i] - alpha * dEdb_l # обновление b_l\n",
    "    for i in range(len(y_l_minus_1)): # итерация по входным каналам\n",
    "        dEdy_l_minus_1 = 0\n",
    "        k = 0\n",
    "        # далее итерация по \"промежуточным\" картам признаков, соответствующим i-тому входному каналу\n",
    "        # сами \"промежуточные\" карты не используются! в вычислениях присутствуют только \"финальные\" карты\n",
    "        # при этом количество \"финальных\" карт (здесь dEdx_l) равно feature_maps, тогда как количество w_l равно feature_maps*y_l_minus_1\n",
    "        # отсюда использование дополнительного \"итератора\" k, таким образом к dEdx_l мы обращаемся с помощью k, а к w_l с помощью j\n",
    "        for j in range(i*feature_maps, (i + 1)*feature_maps):\n",
    "            dEdw_l = convolution_back_dEdw_l(\n",
    "                y_l_minus_1=y_l_minus_1[i], # i-тый входной канал\n",
    "                w_l_shape=w_l[j].shape, # j-тый w_l\n",
    "                dEdx_l=list_of_dEdx_l[k], # k-тый dEdx_l\n",
    "                conv_params=conv_params\n",
    "            )\n",
    "            # через слой y_l_minus_1 проходят суммы показателей со всех карт признаков\n",
    "            dEdy_l_minus_1 += convolution_back_dEdy_l_minus_1(\n",
    "                dEdx_l=list_of_dEdx_l[k], # k-тый dEdx_l\n",
    "                w_l=w_l[j], # j-тый w_l\n",
    "                y_l_minus_1_shape=y_l_minus_1[i].shape, # i-тый входной канал\n",
    "                conv_params=conv_params\n",
    "            )\n",
    "            w_l[j] = w_l[j] - alpha * dEdw_l # обновление w_l\n",
    "            k += 1\n",
    "        list_of_dEdy_l_minus_1.append(dEdy_l_minus_1)\n",
    "    return list_of_dEdy_l_minus_1, w_l, b_l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np # основная библиотека для работы с массивами\n",
    "import matplotlib.pyplot as plt # для построения графиков\n",
    "import PIL # для работы с изображениями\n",
    "import os # для работы с файлами на диске\n",
    "# import model # книга с функциями\n",
    "from tensorflow.examples.tutorials.mnist import input_data # датасет mnist\n",
    "import tensorflow as tf\n",
    "# from pudb import set_trace; set_trace() # для дебага"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# закрепление сидов\n",
    "np.random.seed(0)\n",
    "tf.set_random_seed(0)\n",
    "\n",
    "train_model = False # обучение или тест модели\n",
    "\n",
    "# загрузка датасета\n",
    "image_storage = [] # здесь будут храниться изображения\n",
    "truth_storage = [] # здесь будут храниться ground truth лейблы для изображений\n",
    "dir_storage = [] # директории к изображениям\n",
    "weight_dir = './cnn_weights_mnist.npy'\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "\n",
    "if train_model:\n",
    "    mnist_dataset = mnist.train.images\n",
    "    truth_storage = mnist.train.labels\n",
    "else:\n",
    "    for i in range(55000): # mnist.test сортируется после прохождения по mnist.train (для сравнения результатов с tensorflow)\n",
    "        batch = mnist.train.next_batch(1)\n",
    "    mnist_dataset = mnist.test.images\n",
    "    truth_storage = mnist.test.labels\n",
    "\n",
    "for input_image in mnist_dataset:\n",
    "    image_storage.append(np.reshape(input_image, (28, 28))) # (784,1) -> (28,28)\n",
    "# for i in range(len(mnist_dataset)): # сборка image_storage через next_batch с фиксированным сидом для сравнения результатов с моделью на tensroflow\n",
    "# \tinput_image = mnist.train.next_batch(1)\n",
    "# \timage_storage.append(np.reshape(input_image[0], (28, 28))) # (784,1) -> (28,28)\n",
    "# \ttruth_storage.append(np.reshape(input_image[1], (10,))) # (1,10) -> (10,)\n",
    "dir_storage = ['None' for i in range(len(image_storage))]\n",
    "\n",
    "# первый и последний шаги\n",
    "if train_model:\n",
    "    start_step = model.get_start_step(weight_dir)\n",
    "    end_step = len(image_storage)\n",
    "    len_dataset = 1000 # частота вывода print и сохранения весов (не менять при возобновлении обучения)\n",
    "else:\n",
    "    start_step = 0\n",
    "    end_step = len(image_storage)\n",
    "    len_dataset = 1000\n",
    "\n",
    "# перемешивание датасета приводит к тому, что изображения становятся в последовательность, аналогичную next_batch(1) для данного сида\n",
    "image_storage, truth_storage, dir_storage = model.shuffle_list(image_storage, truth_storage, dir_storage)\n",
    "\n",
    "# параметры сети\n",
    "model_settings = {\n",
    "    'learning_rate':0.01, # коэффициент обучения\n",
    "    'conv_shape_1':(2,2), # размер ядра свертки\n",
    "    'conv_shape_2':(3,3),\n",
    "    'maxpool_shape_1':(2,2), # размер окна макспулинга\n",
    "    'conv_feature_1':5, # количесвто feature maps на выходе функции\n",
    "    'conv_feature_2':20,\n",
    "    'conv_stride_1':2, # величина шага\n",
    "    'conv_stride_2':1,\n",
    "    'maxpool_stride_1':2,\n",
    "    'fc_neurons_1':2000, # количество нейронов в скрытом слое\n",
    "    'conv_fn_1':'relu', # функция активации\n",
    "    'conv_fn_2':'sigmoid',\n",
    "    'fc_fn_1':'sigmoid',\n",
    "    'fc_fn_2':'softmax',\n",
    "    'conv_conv_1':False, # операция конволюции или кросс-корреляции\n",
    "    'conv_conv_2':False,\n",
    "    'maxpool_conv_1':False, # \"конволюция\" или \"корреляция\" для операции макспулинга\n",
    "    'conv_center_1':(0,0), # центр ядра\n",
    "    'conv_center_2':(1,1),\n",
    "    'maxpool_center_1':(0,0)\n",
    "}\n",
    "\n",
    "# параметры для первого слоя конволюции (начальные параметры будут инициализированы во время работы сети)\n",
    "# веса для дообучения сети будут подгружены из файла\n",
    "conv_w_1 = []\n",
    "conv_b_1 = []\n",
    "# параметры для второго слоя конволюции\n",
    "conv_w_2 = []\n",
    "conv_b_2 = []\n",
    "# параметры для первого слоя fc-сети\n",
    "fc_w_1 = np.array([[]])\n",
    "fc_b_1 = np.array([[]])\n",
    "# параметры для второго слоя fc-сети\n",
    "fc_w_2 = np.array([[]])\n",
    "fc_b_2 = np.array([[]])\n",
    "\n",
    "# создание начальных значений с закрепленным сидом для сравнения результатов с моделью на tensorflow\n",
    "# этот участок кода можно просто закомментировать, если нет необходимости в инициализации tensorflow-весов\n",
    "#if not os.path.isfile(weight_dir):\n",
    "#    tf_w1 = tf.truncated_normal([2, 2, 1, 5], stddev=0.1)\n",
    "#    tf_w2 = tf.constant(0.1, shape=[5])\n",
    "#    tf_w3 = tf.truncated_normal([3, 3, 5, 20], stddev=0.1)\n",
    "#    tf_w4 = tf.constant(0.1, shape=[20])\n",
    "#    tf_w5 = tf.truncated_normal([7*7*20, 2000], stddev=0.1)\n",
    "#    tf_w6 = tf.constant(0.1, shape=[2000])\n",
    "#    tf_w7 = tf.truncated_normal([2000, 10], stddev=0.1)\n",
    "#    tf_w8 = tf.constant(0.1, shape=[10])\n",
    "#    with tf.Session() as sess:\n",
    "#        w1, w2, w3, w4, w5, w6, w7, w8 = sess.run([tf_w1, tf_w2, tf_w3, tf_w4, tf_w5, tf_w6, tf_w7, tf_w8])\n",
    "#    w1 = np.reshape(w1, (w1.size,))\n",
    "#    w1 = np.reshape(w1, (5,2,2), order='F')\n",
    "#    for i in range(5):\n",
    "#        conv_w_1.append(w1[i].T)\n",
    "#    w3 = np.reshape(w3, (w3.size,))\n",
    "#    w3 = np.reshape(w3, (5*20,3,3), order='F')\n",
    "#    for i in range(5*20):\n",
    "#        conv_w_2.append(w3[i].T)\n",
    "#    conv_b_1 = w2\n",
    "#    conv_b_2 = w4\n",
    "#    fc_w_1 = w5\n",
    "#    fc_b_1 = w6\n",
    "#    fc_w_2 = w7\n",
    "#    fc_b_2 = w8\n",
    "\n",
    "# загрузка результатов предыдущего обучения из дампов модели (если первое обучение - создаются пустые листы)\n",
    "if train_model:\n",
    "    loss_change = model.get_saved('loss_change', weight_dir)\n",
    "    accuracy_change = model.get_saved('accuracy_change', weight_dir)\n",
    "else:\n",
    "    loss_change = []\n",
    "    accuracy_change = []\n",
    "for step in range(start_step, end_step):\n",
    "    # извлечение изображения из хранилища\n",
    "    image_id = step%len(image_storage) # на каждом шаге обновляются веса для одного изображения\n",
    "    print ('до вывода результатов', str(round((step%len_dataset)*100/len_dataset)) + '%', end=\"\\r\")\n",
    "    input_image = [image_storage[image_id]] # здесь лист, так как convolution_feed на вход принимает лист, состоящий из feature maps\n",
    "    y_true = truth_storage[image_id]\n",
    "    # прямое прохожение сети\n",
    "    # первый конволюционный слой\n",
    "    conv_y_1, conv_w_1, conv_b_1 = model.convolution_feed(\n",
    "        y_l_minus_1=input_image,\n",
    "        w_l=conv_w_1,\n",
    "        w_l_name='conv_w_1', # для подгрузки весов из файла\n",
    "        w_shape_l=model_settings['conv_shape_1'],\n",
    "        b_l=conv_b_1,\n",
    "        b_l_name='conv_b_1',\n",
    "        feature_maps=model_settings['conv_feature_1'],\n",
    "        act_fn=model_settings['conv_fn_1'],\n",
    "        dir_npy=weight_dir,\n",
    "        conv_params={\n",
    "            'convolution':model_settings['conv_conv_1'],\n",
    "            'stride':model_settings['conv_stride_1'],\n",
    "            'center_w_l':model_settings['conv_center_1']\n",
    "        }\n",
    "    )\n",
    "    # слой макспулинга\n",
    "    conv_y_1_mp, conv_y_1_mp_to_conv_y_1 = model.maxpool_feed(\n",
    "        y_l=conv_y_1,\n",
    "        conv_params={\n",
    "            'window_shape':model_settings['maxpool_shape_1'],\n",
    "            'convolution':model_settings['maxpool_conv_1'],\n",
    "            'stride':model_settings['maxpool_stride_1'],\n",
    "            'center_window':model_settings['maxpool_center_1']\n",
    "        }\n",
    "    )\n",
    "    # второй конволюционный слой\n",
    "    conv_y_2, conv_w_2, conv_b_2 = model.convolution_feed(\n",
    "        y_l_minus_1=conv_y_1_mp,\n",
    "        w_l=conv_w_2,\n",
    "        w_l_name='conv_w_2',\n",
    "        w_shape_l=model_settings['conv_shape_2'],\n",
    "        b_l=conv_b_2,\n",
    "        b_l_name='conv_b_2',\n",
    "        feature_maps=model_settings['conv_feature_2'],\n",
    "        act_fn=model_settings['conv_fn_2'],\n",
    "        dir_npy=weight_dir,\n",
    "        conv_params={\n",
    "            'convolution':model_settings['conv_conv_2'],\n",
    "            'stride':model_settings['conv_stride_2'],\n",
    "            'center_w_l':model_settings['conv_center_2']\n",
    "        }\n",
    "    )\n",
    "    # конвертация полученных feature maps в вектор\n",
    "    conv_y_2_vect = model.matrix2vector_tf(conv_y_2)\n",
    "    # первый слой fully connected сети\n",
    "    fc_y_1, fc_w_1, fc_b_1 = model.fc_multiplication(\n",
    "        y_l_minus_1=conv_y_2_vect,\n",
    "        w_l=fc_w_1,\n",
    "        w_l_name='fc_w_1',\n",
    "        b_l=fc_b_1,\n",
    "        b_l_name='fc_b_1',\n",
    "        neurons=model_settings['fc_neurons_1'],\n",
    "        act_fn=model_settings['fc_fn_1'],\n",
    "        dir_npy=weight_dir\n",
    "    )\n",
    "    # второй слой fully connected сети\n",
    "    fc_y_2, fc_w_2, fc_b_2 = model.fc_multiplication(\n",
    "        y_l_minus_1=fc_y_1,\n",
    "        w_l=fc_w_2,\n",
    "        w_l_name='fc_w_2',\n",
    "        b_l=fc_b_2,\n",
    "        b_l_name='fc_b_2',\n",
    "        neurons=len(y_true), # количество нейронов на выходе моледи равно числу классов\n",
    "        act_fn=model_settings['fc_fn_2'],\n",
    "        dir_npy=weight_dir\n",
    "    )\n",
    "    # ошибка модели\n",
    "    fc_error = model.loss_fn(y_true, fc_y_2, feed=True)\n",
    "    # сохранение значений loss и accuracy\n",
    "    loss_change.append(fc_error.sum())\n",
    "    accuracy_change.append(y_true.argmax() == fc_y_2.argmax())\n",
    "    # обратное прохожение по сети\n",
    "    if train_model:\n",
    "        # backprop через loss-функцию\n",
    "        dEdfc_y_2 = model.loss_fn(y_true, fc_y_2, feed=False)\n",
    "        # backprop через второй слой fc-сети\n",
    "        dEdfc_y_1, fc_w_2, fc_b_2 = model.fc_backpropagation(\n",
    "            y_l_minus_1=fc_y_1,\n",
    "            dEdy_l=dEdfc_y_2,\n",
    "            y_l=fc_y_2,\n",
    "            w_l=fc_w_2,\n",
    "            b_l=fc_b_2,\n",
    "            act_fn=model_settings['fc_fn_2'],\n",
    "            alpha=model_settings['learning_rate']\n",
    "        )\n",
    "        # backprop через первый слой fc-сети\n",
    "        dEdfc_y_0, fc_w_1, fc_b_1 = model.fc_backpropagation(\n",
    "            y_l_minus_1=conv_y_2_vect,\n",
    "            dEdy_l=dEdfc_y_1,\n",
    "            y_l=fc_y_1,\n",
    "            w_l=fc_w_1,\n",
    "            b_l=fc_b_1,\n",
    "            act_fn=model_settings['fc_fn_1'],\n",
    "            alpha=model_settings['learning_rate']\n",
    "        )\n",
    "        # конвертация полученного вектора в feature maps\n",
    "        dEdconv_y_2 = model.vector2matrix_tf(\n",
    "            vector=dEdfc_y_0,\n",
    "            matrix_shape=conv_y_2[0].shape # размерность одной из матриц feature map\n",
    "        )\n",
    "        # backprop через второй слой конволюции\n",
    "        dEdconv_y_1_mp, conv_w_2, conv_b_2 = model.convolution_backpropagation(\n",
    "            y_l_minus_1=conv_y_1_mp, # так как слой макспулинга!\n",
    "            y_l=conv_y_2,\n",
    "            w_l=conv_w_2,\n",
    "            b_l=conv_b_2,\n",
    "            dEdy_l=dEdconv_y_2,\n",
    "            feature_maps=model_settings['conv_feature_2'],\n",
    "            act_fn=model_settings['conv_fn_2'],\n",
    "            alpha=model_settings['learning_rate'],\n",
    "            conv_params={\n",
    "                'convolution':model_settings['conv_conv_2'],\n",
    "                'stride':model_settings['conv_stride_2'],\n",
    "                'center_w_l':model_settings['conv_center_2']\n",
    "            }\n",
    "        )\n",
    "        # backprop через слой макспулинга\n",
    "        dEdconv_y_1 = model.maxpool_back(\n",
    "            dEdy_l_mp=dEdconv_y_1_mp,\n",
    "            y_l_mp_to_y_l=conv_y_1_mp_to_conv_y_1,\n",
    "            y_l_shape=conv_y_1[0].shape\n",
    "        )\n",
    "        # backprop через первый слой конволюции\n",
    "        dEdconv_y_0, conv_w_1, conv_b_1 = model.convolution_backpropagation(\n",
    "            y_l_minus_1=input_image,\n",
    "            y_l=conv_y_1,\n",
    "            w_l=conv_w_1,\n",
    "            b_l=conv_b_1,\n",
    "            dEdy_l=dEdconv_y_1,\n",
    "            feature_maps=model_settings['conv_feature_1'],\n",
    "            act_fn=model_settings['conv_fn_1'],\n",
    "            alpha=model_settings['learning_rate'],\n",
    "            conv_params={\n",
    "                'convolution':model_settings['conv_conv_1'],\n",
    "                'stride':model_settings['conv_stride_1'],\n",
    "                'center_w_l':model_settings['conv_center_1']\n",
    "            }\n",
    "        )\n",
    "    # вывод результатов\n",
    "    if len(loss_change)%len_dataset == 0:\n",
    "        print('шаг:', len(loss_change), 'loss:', sum(loss_change[-len_dataset:])/len_dataset, 'accuracy:', sum(accuracy_change[-len_dataset:])/len_dataset)\n",
    "        # сохранение весов\n",
    "        if train_model:\n",
    "            np.save(weight_dir, {\n",
    "                'step':step,\n",
    "                'loss_change':loss_change,\n",
    "                'accuracy_change':accuracy_change,\n",
    "                'conv_w_1':conv_w_1,\n",
    "                'conv_b_1':conv_b_1,\n",
    "                'conv_w_2':conv_w_2,\n",
    "                'conv_b_2':conv_b_2,\n",
    "                'fc_w_1':fc_w_1,\n",
    "                'fc_b_1':fc_b_1,\n",
    "                'fc_w_2':fc_w_2,\n",
    "                'fc_b_2':fc_b_2\n",
    "                }\n",
    "            )\n",
    "    # перемешивание датасета [https://stats.stackexchange.com/questions/272409/mixing-shuffle-order-on-training-set-for-future-epochs]\n",
    "    if train_model and len(loss_change)%len(image_storage) == 0:\n",
    "        image_storage, truth_storage, dir_storage = model.shuffle_list(image_storage, truth_storage, dir_storage)\n",
    "\n",
    "if not train_model:\n",
    "    print('test_loss:', sum(loss_change)/len(loss_change), 'test_accuracy:', sum(accuracy_change)/len(accuracy_change))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
